{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2aa0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrOCRProcessor:\n",
       "- image_processor: ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"processor_class\": \"TrOCRProcessor\",\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 384,\n",
       "    \"width\": 384\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: RobertaTokenizerFast(name_or_path='processor/microsoft/trocr-base-handwritten', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"TrOCRProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from transformers import TrOCRProcessor\n",
    "\n",
    "\n",
    "batch_size=4\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\n",
    "    'processor/microsoft/trocr-base-handwritten')\n",
    "\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86fe24f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'text'],\n",
       "     num_rows: 68000\n",
       " }),\n",
       " {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=34x26>,\n",
       "  'text': '竺航欣'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#准备数据\n",
    "#接下来我们要喂给模型以及裁剪好的，同时又修剪填充至统一大小的图片\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "def f():\n",
    "    dataset = load_from_disk('dataset/data')['train']\n",
    "    for data in dataset:\n",
    "        for box in data['ocr']:\n",
    "            image = data['image'].crop(box['box'])\n",
    "            yield {'image': image, 'text': box['word']}\n",
    "\n",
    "\n",
    "dataset = Dataset.from_generator(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c6ba0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=384x384>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "\n",
    "#填充图片至统一大小384*384\n",
    "def pad(image):\n",
    "    w, h = image.size\n",
    "\n",
    "    ratio = 384 / max(w, h)\n",
    "\n",
    "    w = int(ratio * w)\n",
    "    h = int(ratio * h)\n",
    "\n",
    "    image = image.resize([w, h])\n",
    "\n",
    "    #新建一个全黑的画布\n",
    "    pad = PIL.Image.new('RGB', [384, 384], 'black')\n",
    "    #在（0，0）处黏贴图片\n",
    "    pad.paste(image, [0, 0])\n",
    "\n",
    "    return pad\n",
    "\n",
    "#效果展示\n",
    "pad(PIL.Image.new('RGB', [100, 20], 'green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f441d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17000,\n",
       " (tensor([[[[ 0.8275,  0.8275,  0.8275,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8275,  0.8275,  0.8275,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8275,  0.8275,  0.8275,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [ 0.8431,  0.8431,  0.8431,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8431,  0.8431,  0.8431,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8431,  0.8431,  0.8431,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 0.9373,  0.9373,  0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.9373,  0.9373,  0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.9373,  0.9373,  0.9373,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [ 0.8980,  0.8980,  0.8980,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8980,  0.8980,  0.8980,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.8980,  0.8980,  0.8980,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 1.0000,  1.0000,  1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            ...,\n",
       "            [ 0.9843,  0.9843,  0.9843,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.9843,  0.9843,  0.9843,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [ 0.9843,  0.9843,  0.9843,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.9529,  0.9529,  0.9529,  ...,  0.9059,  0.9059,  0.9059],\n",
       "            [ 0.9529,  0.9529,  0.9529,  ...,  0.9059,  0.9059,  0.9059],\n",
       "            [ 0.9529,  0.9529,  0.9529,  ...,  0.9059,  0.9059,  0.9059],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 0.9608,  0.9608,  0.9608,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            [ 0.9608,  0.9608,  0.9608,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            [ 0.9608,  0.9608,  0.9608,  ...,  0.9922,  0.9922,  0.9922],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
       "            [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
       "            [ 0.8510,  0.8510,  0.8510,  ...,  0.8510,  0.8510,  0.8510],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 0.9216,  0.9216,  0.9216,  ...,  0.9216,  0.9216,  0.9216],\n",
       "            [ 0.9216,  0.9216,  0.9216,  ...,  0.9216,  0.9216,  0.9216],\n",
       "            [ 0.9216,  0.9216,  0.9216,  ...,  0.9216,  0.9216,  0.9216],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
       "  \n",
       "           [[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
       "            ...,\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "            [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]]],\n",
       "         device='cuda:0'),\n",
       "  tensor([[    0,   176,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1],\n",
       "          [    0, 37127, 15389, 23171,     2,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1],\n",
       "          [    0, 47504,  3602, 48823,   711, 36714, 10172,  9357,     2,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1],\n",
       "          [    0, 49026, 20024, 48956, 13859, 37127, 20024, 14292,     2,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1]],\n",
       "         device='cuda:0')))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这部分与定位部分的代码同理，不再赘述\n",
    "def f(data):\n",
    "    image = [i['image'] for i in data]\n",
    "    text = [i['text'] for i in data]\n",
    "\n",
    "    image = [pad(i) for i in image]\n",
    "    pixel_values = processor(image,\n",
    "                             return_tensors='pt').pixel_values.to('cuda')\n",
    "\n",
    "    input_ids = processor.tokenizer(text,\n",
    "                                    truncation=True,\n",
    "                                    padding='max_length',\n",
    "                                    max_length=128,\n",
    "                                    return_tensors='pt').input_ids.to('cuda')\n",
    "\n",
    "    return pixel_values, input_ids\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True,\n",
    "                                     collate_fn=f)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4abb9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76758\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at model/microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-13.0080, -13.0373,  12.0408,  ..., -12.6863, -12.9229, -11.5388],\n",
      "         [-14.1095, -13.4633,  11.1647,  ..., -13.7185, -13.3112, -12.5025],\n",
      "         [-14.3663, -13.9464,  13.2962,  ..., -13.5188, -13.7461, -13.5494],\n",
      "         ...,\n",
      "         [-11.2676, -11.3228,  11.4693,  ..., -11.5510, -11.7379, -10.9230],\n",
      "         [-11.9383, -10.8586,  12.1512,  ..., -11.8008, -11.7588, -11.3165],\n",
      "         [-11.5888, -10.6852,  12.1466,  ..., -12.0939, -11.5667, -11.1521]],\n",
      "\n",
      "        [[-11.2469, -11.3462,  12.4658,  ..., -10.0075, -10.9729, -10.9649],\n",
      "         [-11.6418, -11.2958,  11.0686,  ..., -10.2152, -11.1086, -11.0561],\n",
      "         [-12.4888, -11.9394,  13.5417,  ..., -11.4685, -13.0003, -12.6705],\n",
      "         ...,\n",
      "         [ -9.5082,  -9.5631,  10.0556,  ...,  -8.6511,  -9.4429,  -8.8269],\n",
      "         [ -9.4227,  -9.9553,  10.4200,  ...,  -9.3621,  -9.3669,  -8.8485],\n",
      "         [ -9.9863,  -9.6293,  10.4471,  ...,  -8.7183,  -9.2198,  -8.7553]],\n",
      "\n",
      "        [[-11.9671, -10.3093,   8.2175,  ..., -10.5194, -10.1788,  -9.8795],\n",
      "         [-11.9136, -11.5495,   9.8989,  ..., -11.0363, -10.5410, -10.8446],\n",
      "         [-11.4986, -10.3079,  14.1109,  ..., -10.9623, -10.7646, -11.3067],\n",
      "         ...,\n",
      "         [-11.1044,  -8.8509,   6.9110,  ...,  -9.3523,  -8.9102,  -9.3540],\n",
      "         [-11.5205,  -9.8823,   7.6871,  ..., -10.1261,  -9.5552, -10.0351],\n",
      "         [-11.3055,  -9.4812,   7.5627,  ...,  -9.9732,  -9.9737,  -9.8957]],\n",
      "\n",
      "        [[-11.6030, -11.0034,   9.7332,  ..., -10.9463, -11.7573, -11.8901],\n",
      "         [-11.4275, -11.3213,   9.7446,  ..., -11.0484, -11.4544, -11.8284],\n",
      "         [ -5.9692,  -6.4318,   7.9788,  ...,  -5.8487,  -4.7292,  -4.9563],\n",
      "         ...,\n",
      "         [ -8.8979,  -8.9526,   7.9177,  ...,  -8.6326,  -8.2369,  -9.2566],\n",
      "         [-10.1065,  -9.8151,   8.2730,  ..., -10.1699,  -9.4549, -10.2976],\n",
      "         [-10.1487,  -9.0767,   8.4140,  ...,  -9.5028,  -9.3673, -10.1722]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        from transformers import VisionEncoderDecoderConfig, ViTModel, TrOCRForCausalLM\n",
    "\n",
    "        config = VisionEncoderDecoderConfig.from_pretrained(\n",
    "            'model/microsoft/trocr-base-stage1')\n",
    "        \n",
    "        #config.use_pretrained_backbone = False\n",
    "\n",
    "        self.encoder = ViTModel(config.encoder)\n",
    "        self.decoder = TrOCRForCausalLM(config.decoder)\n",
    "\n",
    "        from transformers import VisionEncoderDecoderModel\n",
    "        parameters = VisionEncoderDecoderModel.from_pretrained(\n",
    "            'model/microsoft/trocr-base-stage1')\n",
    "        self.encoder.load_state_dict(parameters.encoder.state_dict())\n",
    "        self.decoder.load_state_dict(parameters.decoder.state_dict())\n",
    "        del parameters\n",
    "\n",
    "        self.train()\n",
    "        self.to('cuda')\n",
    "\n",
    "    def forward(self, pixel_values, input_ids):\n",
    "        last_hidden_state = self.encoder(pixel_values).last_hidden_state\n",
    "\n",
    "        input_ids_shifted = torch.zeros_like(input_ids)\n",
    "        input_ids_shifted[:, 1:] = input_ids[:, :-1].clone()\n",
    "        input_ids_shifted[:, 0] = processor.tokenizer.cls_token_id\n",
    "\n",
    "        return self.decoder(input_ids=input_ids_shifted,\n",
    "                            encoder_hidden_states=last_hidden_state).logits\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(model(*next(iter(loader))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2225fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是我自己写的用opencv查看图片的代码\n",
    "'''\n",
    "def show(image):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    #图片还原\n",
    "    image = image - image.min()\n",
    "    image = image / image.max() * 255\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = np.uint8(image.to('cpu').numpy())\n",
    "\n",
    "\n",
    "    image=cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow(\"image\",image)\n",
    "    \n",
    "    \n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "show(torch.randn(3, 100, 100))\n",
    "'''\n",
    "#这是导写的代码\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def show(image):\n",
    "    image = image - image.min()\n",
    "    image = image / image.max() * 255\n",
    "    image = image.permute(1, 2, 0)\n",
    "    image = np.uint8(image.to('cpu').numpy())\n",
    "    image = PIL.Image.fromarray(image, 'RGB')\n",
    "\n",
    "    plt.figure(figsize=(1.5, 1.5))\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "show(torch.randn(3, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8932385c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Insurance Institute for</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(input_ids):\n",
    "    input_ids = input_ids.to('cpu').tolist()\n",
    "\n",
    "    if processor.tokenizer.sep_token_id in input_ids:\n",
    "        idx = input_ids.index(processor.tokenizer.sep_token_id) + 1\n",
    "        input_ids = input_ids[:idx]\n",
    "\n",
    "    return processor.tokenizer.decode(input_ids)\n",
    "\n",
    "\n",
    "decode(torch.LongTensor([0, 19751, 12590, 2534, 13, 2, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e14ac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 10.870011329650879\n",
      "<s>27</s>\n",
      "</s>\n",
      "0 4000 0.7680874466896057\n",
      "<s>937589197712198101</s>\n",
      "<s><s>758919771219801</s>\n",
      "0 8000 1.3053677082061768\n",
      "<s>黑龙江省双鸭山市友谊县</s>\n",
      "<s><s>��龙江省叉鸿帱�市</s>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(\n\u001b[0;32m      3\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m15\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (pixel_values, input_ids) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loader):\n\u001b[0;32m      7\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(pixel_values, input_ids)\n\u001b[0;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), input_ids\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m, in \u001b[0;36mf\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m [pad(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m image]\n\u001b[1;32m----> 6\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(text,\n\u001b[0;32m     10\u001b[0m                                 truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m                                 padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                 max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                 return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pixel_values, input_ids\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\trocr\\processing_trocr.py:84\u001b[0m, in \u001b[0;36mTrOCRProcessor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either an `images` or `text` input to process.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\image_processing_utils.py:551\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m    550\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:237\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m validate_preprocess_arguments(\n\u001b[0;32m    226\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[0;32m    227\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[0;32m    240\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:237\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    225\u001b[0m validate_preprocess_arguments(\n\u001b[0;32m    226\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[0;32m    227\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scaled_image(images[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m do_rescale:\n\u001b[0;32m    240\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt looks like you are trying to rescale already rescaled images. If the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\image_utils.py:172\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_valid_image(img):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_vision_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(img)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_numpy(img)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\utils\\import_utils.py:748\u001b[0m, in \u001b[0;36mis_vision_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m    750\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\importlib\\metadata.py:530\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    524\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \n\u001b[0;32m    526\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\importlib_metadata\\__init__.py:450\u001b[0m, in \u001b[0;36mDistribution.version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    449\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVersion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\importlib_metadata\\__init__.py:428\u001b[0m, in \u001b[0;36mDistribution.metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmetadata\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _meta\u001b[38;5;241m.\u001b[39mPackageMetadata:\n\u001b[0;32m    422\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the parsed metadata for this Distribution.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    The returned object will have keys that name the various bits of\u001b[39;00m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;124;03m    metadata.  See PEP 566 for details.\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    427\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 428\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMETADATA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPKG-INFO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    430\u001b[0m         \u001b[38;5;66;03m# This last clause is here to support old egg-info files.  Its\u001b[39;00m\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;66;03m# effect is to just end up using the PathDistribution's self._path\u001b[39;00m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;66;03m# (which points to the egg-info file) attribute unchanged.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    434\u001b[0m     )\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _adapters\u001b[38;5;241m.\u001b[39mMessage(email\u001b[38;5;241m.\u001b[39mmessage_from_string(text))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\importlib_metadata\\__init__.py:762\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    756\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    757\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    761\u001b[0m     ):\n\u001b[1;32m--> 762\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:1236\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;124;03m    Open the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1236\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1237\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:1222\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_closed()\n\u001b[1;32m-> 1222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m               \u001b[49m\u001b[43mopener\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\pathlib.py:1078\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_opener\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, flags, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0o666\u001b[39m):\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1078\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "#我只训练了一个epoch就停止了，希望有更好装备的同学继续训练，如果可以的话，在评论区分享一下完整训练后的模型，谢谢了\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(\n",
    "    ignore_index=processor.tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (pixel_values, input_ids) in enumerate(loader):\n",
    "        logits = model(pixel_values, input_ids)\n",
    "\n",
    "        loss = criterion(logits.flatten(0, 1), input_ids.flatten())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 4000 == 0:\n",
    "            print(epoch, i, loss.item())\n",
    "            testi = random.randint(0, batch_size-1)\n",
    "            ###由于环境问题注释掉了\n",
    "            #show(pixel_values[testi])\n",
    "            print(decode(input_ids[testi]))\n",
    "            print(decode(logits[testi].argmax(1)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30611c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.to('cpu'), 'model/文字识别.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7beeff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
